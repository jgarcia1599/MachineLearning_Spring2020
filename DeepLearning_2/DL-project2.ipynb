{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network (GAN)\n",
    "Since GANs were introduced in 2014 by Google Researcher Ian Goodfellow, the tech has been widely adopted in image generation and transfer. After some early wiry failures, GANs have made huge breakthroughs and can now produce highly convincing fake images of animals, landscapes, human faces, etc. Researchers know what GANs can do, however a lack of transparency in their inner workings means GAN improvement is still achieved mainly through trial-and-error. This allows only limited control over the synthesized images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Packages\n",
    "Let's first import the necessary packages,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-692095057d14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## GPU Device Configuration\n",
    "Then, we set up and configure our computational devices: \n",
    "Whether we use GPU or perform the calculation on CPU.\n",
    "we use the torch.devices() and torch.cude.is_available() functions to configure our computational devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Configuration\n",
    "### hyper parameters\n",
    "We then set up and hyper parameters that need for the our model.\n",
    "we need to define several hyper parameters for our model:\n",
    "1. latent size\n",
    "2. hidden size\n",
    "3. input image size\n",
    "4. numbper of epoches\n",
    "5. batch size\n",
    "6. out put directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory if not exists\n",
    "using os.path.exists() to check whether it is exist\n",
    "using os.makedires to create a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##  Image processing\n",
    "Then, we define a image preprocessing object that our dataloader can directly use this object to preprocess our data\n",
    "We use the pytorch API to preform the data processing.\n",
    "1. Use transforms.Compose()\n",
    "2. Use transforms.CenterCrop(160) \n",
    "3. Use transforms.Scale(64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##  Data Loading\n",
    "Next, we are going to load our data. \n",
    "### First, we need to prepare our data:\n",
    "#### we use the following command to download our data:\n",
    "1. apt-get install p7zip-full # ubuntu\n",
    "\n",
    "2. brew install p7zip # Mac\n",
    "\n",
    "3. python download.py\n",
    "-----\n",
    "### We first import necessary librarys for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.datasets as dset\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We first define several helper functions that can help use to load each item in the dataset .\n",
    "1. We first create a list that contains all image files.    '.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',.\n",
    "\n",
    "2. We then define a function called is_image_file() which takes the file name as the input:\n",
    "    <br />a. We 1 if it is a valid image file.\n",
    "    <br />b. Otherwise we return 0.\n",
    "    \n",
    "3. We next define a make_dataset() function which takes a file path as the input:\n",
    "    <br />a. We go over the path\n",
    "    <br />b. If it is a valid img file, store it to a list\n",
    "    <br />c. Return the list\n",
    "    \n",
    "4. Finally, we create a function that is called default_loader()\n",
    "    <br />a. that will open the image and conver it to the RGB using Image.open() and convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We then can use those helper functions to create our dataloader that load each item.  This function is called ImageFloder(data.Dataset) \n",
    "1. This function is initlize with root, transform, traget transform and loader\n",
    "    <br /> a. get all imgs using the correct func we define above\n",
    "    <br /> b. if no valide imgs: raise an proper error\n",
    "    <br /> c. print the length of valid data\n",
    "\n",
    "\n",
    "\n",
    "2. We need to define a __getitem__() function that take index as input\n",
    "    <br /> a. for the \"index\" element in the img list\n",
    "         i. if the transform is not none\n",
    "             we transform the img using the transofrm\n",
    "         ii. if the traget transform is not none:\n",
    "             same.\n",
    "    <br /> b. return the img and target.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. We deine a __len__() function that retrun the length of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We then define our data loader get_loader()\n",
    "#### This func has 6 arguments\n",
    "1. root\n",
    "2. split\n",
    "3. batch size\n",
    "4. scale size\n",
    "5. number of workers\n",
    "6. shuffle\n",
    "\n",
    "\n",
    "\n",
    "<br />We first store the return value of os.path.basename of root to a variable\n",
    "<br />Then we store the path to the image root\n",
    "        <br /> 1. if the dataset_name is in CelebA:\n",
    "        <br /> 2. then, we using the ImageFolder object we define above with transform.CenterCrop(160) to store the data\n",
    "        <br /> 3. otherwise, we do not add the transfrom.CenterCrop when storing the data\n",
    "<br />Finally, we create a data_loade using torch.utils.data.DataLoader() with proper parameters and set the proper shape of this data_loader using data_loade.shape=?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the function above to load the data to the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##  Network\n",
    "Next, we are going to design our GAN\n",
    "We use the pytorch function nn.Sequential() to stack several layers as well as activation functions\n",
    "### First, we need to create our discriminator\n",
    "1. We need one input layer, one hidden layers and one out put layer. All of them are defined using nn.Linear() with proper input dim and out dim\n",
    "\n",
    "\n",
    "2. We adopt nn.LeakyReLU(0.2) as activation layer for the input and hidden layer. \n",
    "\n",
    "\n",
    "3. We use nn.Sigmoid() activation function for the output layer\n",
    "\n",
    "### Next, we are going to define our generator \n",
    "1. We need one input layer, one hidden layers and one out put layer. All of them are defined using nn.Linear() with proper input dim and out dim\n",
    "\n",
    "\n",
    "2. We adopt nn.ReLU() as activation layer for the input and hidden layer. \n",
    "\n",
    "\n",
    "3. We use nn.Tanh() activation function for the output layer\n",
    "\n",
    "\n",
    "#### Please First construct the generator module as follow:\n",
    "1.\tThe input of the first linear layer is the latent vector size, output of the first layer is 256\n",
    "2.\tThen followed by a ReLU layer.\n",
    "3.\tThe input of the second layer is the 256 and output channel is 512\n",
    "4.\tFollowed by the ReLU layer\n",
    "5.\tThe input of the third layer is the 512 and output channel is 1024\n",
    "6.\tFollowed by the ReLU layer\n",
    "7.\tThe input of the fourth layer is the 1024 and output channel is 1024\n",
    "8.\tFollowed by the ReLU layer\n",
    "9.\tThe input of the final layer is the 1024 and output channel is the image size.\n",
    "10.\tThe Tanh is activation function.\n",
    "\n",
    "#### Please First construct the discriminator module as follow:\n",
    "1.\tThe input of the first linear layer is the image size, output of the first layer is 256\n",
    "2.\tThen followed by a LeakyReLU layer.\n",
    "3.\tThe input of the second layer is the 256 and output channel is 512\n",
    "4.\tFollowed by the leakyReLU layer\n",
    "5.\tThe input of the third layer is the 512 and output channel is 512\n",
    "6.\tFollowed by the leakyReLU layer\n",
    "7.\tThe input of the final layer is the 512 and output channel is 1\n",
    "8.\tThe sigmoid is activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "\n",
    "\n",
    "# Generator \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we send the network to the target device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, We set the Binary cross entropy loss and optimizer with proper netwrok parameters and learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##  Training\n",
    "Then, we are going to train our Network\n",
    "### We first starting with two helper function\n",
    "1. We frist implement the denorm function using clamp() api from pytorch please refer https://pytorch.org/docs/stable/torch.html?highlight=clamp#torch.clamp\n",
    "2. We define a function that reset all the gradient of the optimziers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training\n",
    "1. we first store the total steps which is equal to the length of data_loader\n",
    "2. for each epoch\n",
    "    <br/> a. for each element index and element in the data loader\n",
    "        i. we reshape the input data to (batch_sizeï¼Œ-1) and send to the proper device\n",
    "        ii. then we create the real and fake labels which are later used as input for the BCE loss using torch.ones(batch_size, 1).to(device) and torch.zeros(batch_size, 1).to(device)\n",
    "        iii. then we train the descriminator\n",
    "            A. feedforward and store the predictions of discriminator\n",
    "            B. compute BCE_Loss using real images and store the loss\n",
    "            C. random init a latent code z\n",
    "            D. feedforward and store the predictions of generator\n",
    "            E. feed the predictions to the descriminator and sotre the prediction\n",
    "            F. compute BCELoss using fake images and store the loss\n",
    "            G. perofrm the backprop using losses after that reset the gradient of optimzier.\n",
    "        iv. then we train the generator\n",
    "            A. using torch.randn(batch_size, latent_size).to(device) to init a z\n",
    "            B. feedforward and store the predictions of generator\n",
    "            C. feed the predictions to the descriminator and sotre the prediction\n",
    "            D. compute BCE_Loss using real images and store the loss\n",
    "            F. perofrm the backprop using losses after that reset the gradient of optimzier.\n",
    "        v. some centain period, we prient the log with proper info\n",
    "    <br/> b. we store all real image with shape (images.size(0), 1, 28, 28) inoder for the further comparision only once\n",
    "    <br/> c. we save the fake image with image shape (images.size(0), 1, 28, 28) and denorm() function \n",
    "3. Save the model checkpoints using torch.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
