{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Part1_LinearRegression_jr.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm8Xmtoy6TTL",
        "colab_type": "text"
      },
      "source": [
        "# Simple Linear versus Ridge Regression "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C6FopWK6TTP",
        "colab_type": "text"
      },
      "source": [
        "## Requirement 1:  Getting, understanding, and preprocessing the dataset\n",
        "\n",
        "We first import the standard libaries and some libraries that will help us scale the data and perform some \"feature engineering\" by transforming the data into $\\Phi_2({\\bf x})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBgHUpXx6TTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.linear_model\n",
        "from sklearn.model_selection import KFold\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import collections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEI_CqjyAnl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHEEXHAP6TTW",
        "colab_type": "text"
      },
      "source": [
        "###  Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "astUD2iD6TTX",
        "colab_type": "code",
        "outputId": "90b9a6ef-2016-4891-c9de-0221b794e02a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Import the boston dataset from sklearn\n",
        "# Load dataset to some variable \n",
        "# boston_data = .....\n",
        "boston_data = load_boston()\n",
        "boston_data.keys()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F4bul8ZLokB",
        "colab_type": "text"
      },
      "source": [
        "Exploration of the Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCiZeSuxLcW0",
        "colab_type": "code",
        "outputId": "14ae25b3-195b-4362-ba17-b44b83532cf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "print(\"Shape of the data set: \")\n",
        "print(boston_data.data.shape)\n",
        "print(\"Actual Data Set: \")\n",
        "print(boston_data.data)\n",
        "# boston_data.data\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the data set: \n",
            "(506, 13)\n",
            "Actual Data Set: \n",
            "[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]\n",
            " [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]\n",
            " [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]\n",
            " ...\n",
            " [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]\n",
            " [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]\n",
            " [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVYb9ujhLF13",
        "colab_type": "code",
        "outputId": "f643cec8-e73e-419f-9334-f4943bbe8b47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"Features of the data set to run regression on: \")\n",
        "boston_data.feature_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features of the data set to run regression on: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
              "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYEAZFaRLcBc",
        "colab_type": "code",
        "outputId": "28bf2882-4980-4463-9605-e0c05cf325c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "boston_data.DESCR"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNcL05w2NiFR",
        "colab_type": "code",
        "outputId": "e9444447-cd97-4b7a-ec46-460b474b3d6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "boston_pd = pd.DataFrame(boston_data.data)\n",
        "print(boston_pd.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        0     1     2    3      4   ...   8      9     10      11    12\n",
            "0  0.00632  18.0  2.31  0.0  0.538  ...  1.0  296.0  15.3  396.90  4.98\n",
            "1  0.02731   0.0  7.07  0.0  0.469  ...  2.0  242.0  17.8  396.90  9.14\n",
            "2  0.02729   0.0  7.07  0.0  0.469  ...  2.0  242.0  17.8  392.83  4.03\n",
            "3  0.03237   0.0  2.18  0.0  0.458  ...  3.0  222.0  18.7  394.63  2.94\n",
            "4  0.06905   0.0  2.18  0.0  0.458  ...  3.0  222.0  18.7  396.90  5.33\n",
            "\n",
            "[5 rows x 13 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeUpTYl9WA71",
        "colab_type": "text"
      },
      "source": [
        "###Requirement 2: Features\n",
        "Below is a brief explanation of the different characteristics of the data set. We have 13 features, such as crime per capita (CRIM) and INDUS (proportion of non-retail business acres per town) , that will be used to determine house prices in Boston. Our goal for both the ridge regression model and the linear regression model is to find the best w for each feature in order to predict house prices. Such models will be trained with a dataset of 506 data points, and we will use the closed form solution for both models to train our model and determine our w values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwwrrMT46TTa",
        "colab_type": "code",
        "outputId": "ded19593-f108-47c1-9d69-9f96ade23051",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#  Create X and Y variables - X holding the .data and Y holding .target \n",
        "X = boston_data.data\n",
        "y = boston_data.target\n",
        "#  Reshape Y to be a rank 2 matrix using y.reshape()\n",
        "y = y.reshape(X.shape[0],1)\n",
        "# Observe the number of features and the number of labels\n",
        "print('The number of features is: ', X.shape[1])\n",
        "# Printing out the features\n",
        "print('The features: ', boston_data.feature_names)\n",
        "# The number of examples\n",
        "print('The number of examples in ourdataset: ', X.shape[0])\n",
        "# Observing the first 2 rows of the data\n",
        "print(\"First two rows of data: \")\n",
        "print(X[0:2])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of features is:  13\n",
            "The features:  ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
            " 'B' 'LSTAT']\n",
            "The number of examples in ourdataset:  506\n",
            "First two rows of data: \n",
            "[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
            "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
            "  4.9800e+00]\n",
            " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
            "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
            "  9.1400e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_DmMrRJ6TTk",
        "colab_type": "text"
      },
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xlPwti96TTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the get_coeff_ridge_normaleq function. Use the normal equation method.\n",
        "# Return w values\n",
        "\n",
        "\n",
        "\n",
        "def get_coeff_ridge_normaleq(X_train, y_train, alpha):\n",
        "    # use np.linalg.pinv(...)\n",
        "    # W = ((X_T . X + alpha * I )^-1) . X_T . y\n",
        "    bias = np.ones((X_train.shape[0],1))\n",
        "    X_train = np.c_[X_train,bias]\n",
        "    X_T = X_train.transpose()\n",
        "    X_T_dot_X = X_T.dot(X_train)\n",
        "\n",
        "    alpha_times_identity = alpha * np.identity (X_T_dot_X.shape[1])\n",
        "    temp_1 = np.linalg.pinv(X_T_dot_X + alpha_times_identity)\n",
        "    temp_2 = temp_1.dot(X_T)\n",
        "    w = temp_2.dot(y_train)\n",
        "\n",
        "\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2sNK9yA6TTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the get_coeff_ridge_normaleq function. Use the normal equation method.\n",
        "# Return w values\n",
        "\n",
        "def get_coeff_linear_normaleq(X_train, y_train):\n",
        "    bias = np.ones((X_train.shape[0],1))\n",
        "    X_train= np.c_[X_train,bias]\n",
        "    w = np.linalg.inv(X_train.T.dot(X_train)).dot(X_train.T).dot(y_train)\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr5M4ATR6TTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the evaluate_err_ridge function.\n",
        "# Return the train_error and test_error values\n",
        "\n",
        "\n",
        "def evaluate_err(X_train, X_test, y_train, y_test, w): \n",
        "#     pred_train=prediction using w and X_tran+np.mean(y_train)\n",
        "#     pred_test=prediction using w and X_test\n",
        "#     remember to add the mean back\n",
        "\n",
        "    bias = np.ones((X_train.shape[0],1))\n",
        "    X_train = np.c_[X_train,bias]\n",
        "    pred_train = X_train.dot(w)\n",
        "    testbias = np.ones((X_test.shape[0],1))\n",
        "    X_test = np.c_[X_test,testbias]  \n",
        "    pred_test = X_test.dot(w) \n",
        "    train_error = np.square(np.subtract(y_train,pred_train)).mean()\n",
        "    test_error = np.square(np.subtract(y_test,pred_test)).mean()\n",
        "    \n",
        "    return train_error, test_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmADqW1v6TTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finish writting the k_fold_cross_validation function. \n",
        "# Returns the average training error and average test error from the k-fold cross validation\n",
        "# Sklearns K-Folds cross-validator: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "\n",
        "def k_fold_cross_validation(k, X, y, alpha,model_type):\n",
        "    kf = KFold(n_splits=k, random_state=21, shuffle=True)\n",
        "    total_E_val_test = 0\n",
        "    total_E_val_train = 0\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        \n",
        "        # Centering the data so we do not need the intercept term (we could have also chose w_0=average y value)\n",
        "        # Subtract y_train_mean from y_train and y_test\n",
        "        y_train_mean = np.mean(y_train)\n",
        "        y_train = y_train - y_train_mean\n",
        "        y_test = y_test - y_train_mean\n",
        "        \n",
        "        # Scaling the data matrix\n",
        "        # Using scaler=preprocessing.StandardScaler().fit(...)\n",
        "        # And scaler.transform(...)\n",
        "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "        X_train = scaler.transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "        # Determine the training error and the test error\n",
        "        # Use get_coeff_linear_normaleq or get_coeff_ridge_normaleq to get w\n",
        "        # And use evaluate_err()\n",
        "        if model_type == \"linear\":\n",
        "          w_linear = get_coeff_linear_normaleq(X_train, y_train)\n",
        "          temp1,temp2 = evaluate_err(X_train,X_test, y_train, y_test ,w_linear)\n",
        "          total_E_val_test += temp2\n",
        "          total_E_val_train +=temp1\n",
        "\n",
        "        else:\n",
        "          w_ridge = get_coeff_ridge_normaleq( X_train, y_train , alpha)\n",
        "          temp1,temp2= evaluate_err(X_train,X_test,y_train,y_test,w_ridge)\n",
        "          total_E_val_test += temp2\n",
        "          total_E_val_train += temp1\n",
        "       ##############\n",
        "\n",
        "    #compute average for all Ks\n",
        "    total_E_val_test = total_E_val_test/k\n",
        "    total_E_val_train = total_E_val_train/k\n",
        "    return  total_E_val_test, total_E_val_train\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr6uQ4RcS68H",
        "colab_type": "text"
      },
      "source": [
        "###Requirement 3\n",
        "In the cells below,  both the linear regression model and ridge regression model is implemented using their respective closed form solutions. In order to evaluate the performance of the models,a k-cross validation will be performed, a statistical method that divides the subset into K subparts, training the model with K-1 and testing it with K. 10 proved to be a really good number for k to partition the data set by.  After several rounds of random data partitioning and validation, the k-cross validation reports an an error of 21% during training and 23% during testing for the linear regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzqibcos6TTw",
        "colab_type": "code",
        "outputId": "09be114c-f13c-44ef-aaaa-a40dd1920986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# print the error for the both linear regression and ridge regression\n",
        "# the error should include both training error and testing error\n",
        "\n",
        "\n",
        "#Linear\n",
        "total_E_val_test_linear, total_E_val_train_linear = k_fold_cross_validation(10,X,y,0,\"linear\")\n",
        "print(\"Average Linear Test Error: \",total_E_val_test_linear)\n",
        "print(\"Average Linear Train Error: \",total_E_val_train_linear)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Linear Test Error:  23.63606860542814\n",
            "Average Linear Train Error:  21.806183575851062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgJiDz2qUN3m",
        "colab_type": "text"
      },
      "source": [
        "###Requirement 4 and 5\n",
        "\n",
        "The same procedure will be done for the ridge regression model and validating such using the k-crosss validation procedure. In addition, different lambda values, the coefficient that applies the penalty for the ridge model, will be tested in order to determine the most performant one. Below are the errors for the different lambda, and the one with the least error turned out to be alpha = 10, with a Test Error of 23.68% and  Train Error:21.89"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_D9GWtE6TTz",
        "colab_type": "code",
        "outputId": "70ee466e-0331-4530-e789-51f1b21368c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# use the model to predict the new test case.\n",
        "\n",
        "alpha_values = np.logspace(1, 7, num=13)\n",
        "for i in alpha_values:\n",
        "  total_E_val_test_ridge, total_E_val_train_ridge = k_fold_cross_validation(10,X,y,i,\"ridge\")\n",
        "  print(\"---------------------\"+str(i)+\"---------------------------------\")\n",
        "  print(\"Test Error: \"+str(total_E_val_test_ridge)+\" Train Error: \"+str(total_E_val_train_ridge))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------10.0---------------------------------\n",
            "Test Error: 23.68858300163874 Train Error: 21.892901157570186\n",
            "---------------------31.622776601683793---------------------------------\n",
            "Test Error: 24.01784029738615 Train Error: 22.285444055697496\n",
            "---------------------100.0---------------------------------\n",
            "Test Error: 25.29385255369514 Train Error: 23.725488384882077\n",
            "---------------------316.22776601683796---------------------------------\n",
            "Test Error: 29.457296222896787 Train Error: 28.16655409854012\n",
            "---------------------1000.0---------------------------------\n",
            "Test Error: 39.48949335939869 Train Error: 38.53214872734357\n",
            "---------------------3162.2776601683795---------------------------------\n",
            "Test Error: 54.64719107794949 Train Error: 54.00702649955449\n",
            "---------------------10000.0---------------------------------\n",
            "Test Error: 69.71851750884511 Train Error: 69.2597817132532\n",
            "---------------------31622.776601683792---------------------------------\n",
            "Test Error: 78.92162110435103 Train Error: 78.5307424362927\n",
            "---------------------100000.0---------------------------------\n",
            "Test Error: 82.7724042894599 Train Error: 82.40325012491367\n",
            "---------------------316227.7660168379---------------------------------\n",
            "Test Error: 84.11748625821778 Train Error: 83.75514887733652\n",
            "---------------------1000000.0---------------------------------\n",
            "Test Error: 84.5569941832673 Train Error: 84.19680334862025\n",
            "---------------------3162277.6601683795---------------------------------\n",
            "Test Error: 84.69744416333866 Train Error: 84.33793107733854\n",
            "---------------------10000000.0---------------------------------\n",
            "Test Error: 84.74200651304514 Train Error: 84.38270764054002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv69_o8USnpR",
        "colab_type": "text"
      },
      "source": [
        "###Requirement 6: Polynomial Features\n",
        "In order to improve the results presented above, both the ridge regression and linear regression models will be evaluated using the Polynomial Features of D = 2. This will hopefully predict housing prices better as it will better model any non-linear relationships presented by some features in the data set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f453cRIqpbw",
        "colab_type": "code",
        "outputId": "600a16de-b16b-4dcc-9e7a-ec5034f422ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Create a PolynomialFeatures object with degree = 2. Using PolynomialFeatures(degree=2)\n",
        "# Transform X and save it into X_2 using poly.fit_transform(X)\n",
        "# Simply copy Y into Y_2 \n",
        "\n",
        "poly =PolynomialFeatures(degree = 2)\n",
        "X_2 = poly.fit_transform(X)\n",
        "y_2 = y\n",
        "print(X)\n",
        "print(X_2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]\n",
            " [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]\n",
            " [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]\n",
            " ...\n",
            " [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]\n",
            " [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]\n",
            " [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]\n",
            "[[1.00000000e+00 6.32000000e-03 1.80000000e+01 ... 1.57529610e+05\n",
            "  1.97656200e+03 2.48004000e+01]\n",
            " [1.00000000e+00 2.73100000e-02 0.00000000e+00 ... 1.57529610e+05\n",
            "  3.62766600e+03 8.35396000e+01]\n",
            " [1.00000000e+00 2.72900000e-02 0.00000000e+00 ... 1.54315409e+05\n",
            "  1.58310490e+03 1.62409000e+01]\n",
            " ...\n",
            " [1.00000000e+00 6.07600000e-02 0.00000000e+00 ... 1.57529610e+05\n",
            "  2.23851600e+03 3.18096000e+01]\n",
            " [1.00000000e+00 1.09590000e-01 0.00000000e+00 ... 1.54802902e+05\n",
            "  2.54955600e+03 4.19904000e+01]\n",
            " [1.00000000e+00 4.74100000e-02 0.00000000e+00 ... 1.57529610e+05\n",
            "  3.12757200e+03 6.20944000e+01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36Wq4tviqusY",
        "colab_type": "code",
        "outputId": "f2a32392-2956-444e-c40f-7bb1a3256ac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# the shape of X_2 and Y_2 - should be (506, 105) and (506, 1) respectively\n",
        "print(X_2.shape)\n",
        "print(y_2.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(506, 105)\n",
            "(506, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eygbukgG6TT1",
        "colab_type": "code",
        "outputId": "34ead38e-a9ca-4373-d0c6-9f23cbc9b596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "print(\"Ridge Regression Error Using Polynomial Features:\")\n",
        "for i in alpha_values:\n",
        "  total_E_val_test_ridge, total_E_val_train_ridge = k_fold_cross_validation(10,X_2,y_2,i,\"ridge\")\n",
        "  print(\"---------------------\"+str(i)+\"---------------------------------\")\n",
        "  print(\"Test Error: \"+str(total_E_val_test_ridge)+\" Train Error: \"+str(total_E_val_train_ridge))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ridge Regression Error Using Polynomial Features:\n",
            "---------------------10.0---------------------------------\n",
            "Test Error: 13.476138001136254 Train Error: 10.049055874118876\n",
            "---------------------31.622776601683793---------------------------------\n",
            "Test Error: 15.8296019690513 Train Error: 12.751706269046773\n",
            "---------------------100.0---------------------------------\n",
            "Test Error: 18.980018815009952 Train Error: 16.2226905932108\n",
            "---------------------316.22776601683796---------------------------------\n",
            "Test Error: 22.068692308062726 Train Error: 19.700253646409855\n",
            "---------------------1000.0---------------------------------\n",
            "Test Error: 26.21847559440485 Train Error: 24.28745798310889\n",
            "---------------------3162.2776601683795---------------------------------\n",
            "Test Error: 34.580266018800174 Train Error: 33.08666831985053\n",
            "---------------------10000.0---------------------------------\n",
            "Test Error: 47.78952058756749 Train Error: 46.761999359414084\n",
            "---------------------31622.776601683792---------------------------------\n",
            "Test Error: 62.646288813443356 Train Error: 62.009001916836056\n",
            "---------------------100000.0---------------------------------\n",
            "Test Error: 74.73795160641833 Train Error: 74.28758894586741\n",
            "---------------------316227.7660168379---------------------------------\n",
            "Test Error: 81.07998618951191 Train Error: 80.69255135236737\n",
            "---------------------1000000.0---------------------------------\n",
            "Test Error: 83.53524235105219 Train Error: 83.16723569778434\n",
            "---------------------3162277.6601683795---------------------------------\n",
            "Test Error: 84.36776579104057 Train Error: 84.00579580096273\n",
            "---------------------10000000.0---------------------------------\n",
            "Test Error: 84.63708052069056 Train Error: 84.2770062647736\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfaTLoHYA5Ed",
        "colab_type": "code",
        "outputId": "c18afdb0-bfa7-4874-b917-3c8049078063",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Linear\n",
        "print(\"Linear Error Using Polynomial Features\")\n",
        "total_E_val_test_linear, total_E_val_train_linear = k_fold_cross_validation(10,X_2,y_2,0,\"linear_regression\")\n",
        "print(\"Average Linear Test Error: \",total_E_val_test_linear)\n",
        "print(\"Average Linear Train Error: \",total_E_val_train_linear)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Error Using Polynomial Features\n",
            "Average Linear Test Error:  11.854968234646984\n",
            "Average Linear Train Error:  5.808820816012461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm4k8synYsdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UShIP1SrYxhi",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6GVQEKjVV3K",
        "colab_type": "text"
      },
      "source": [
        "### Requirement 7: Which model performed best?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMKmtH7BXhxT",
        "colab_type": "text"
      },
      "source": [
        "After predicting the data using both the standard linear regression  model and the ridge regression model, the linear regression model performed best for both the standard X and the polynomial X_2. Creating the  polynomial features of degree 2 resulted in a more performant linear regression model  with an 5.8% error during testing and 11.85% error during training. This was surprising, as the data was both centered and normalized before performing the k-cross validation step which might indicate that the model was finetuned enough. However, the polynomial features revealed that there were some non linear relationships between the features that needeed to be modeled with a polynomial curve instead of a straight line.\n",
        "\n",
        "\n"
      ]
    }
  ]
}